{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4221af7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark spark-nlp pandas matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a42db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf, lower, trim, regexp_replace\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DeBertaEmbeddings, SentenceEmbeddings\n",
    "\n",
    "# 1. Khởi tạo Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# 2. Đọc dữ liệu JSONL\n",
    "input_file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "df = spark.read.option(\"multiLine\", False).json(input_file_path)\n",
    "\n",
    "user_questions = df.select(explode(\"messages\").alias(\"msg\")) \\\n",
    "    .filter(col(\"msg.role\") == \"user\") \\\n",
    "    .select(col(\"msg.content\").alias(\"text\")) \\\n",
    "    .filter(col(\"text\").isNotNull())\n",
    "\n",
    "# 3. Pipeline NLP\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = DeBertaEmbeddings.pretrained(\"deberta_embeddings_spm_vie\", \"vie\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"word_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")  # hoặc \"CLS\" nếu có\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    sentence_embeddings\n",
    "])\n",
    "\n",
    "model = pipeline.fit(user_questions)\n",
    "embedded_data = model.transform(user_questions)\n",
    "\n",
    "# 4. Trích xuất vector thành cột features\n",
    "def extract_vector(annot):\n",
    "    if annot and isinstance(annot, list) and 'embeddings' in annot[0]:\n",
    "        return Vectors.dense(annot[0]['embeddings'])\n",
    "    return Vectors.dense([0.0] * 768)\n",
    "\n",
    "extract_vector_udf = udf(extract_vector, VectorUDT())\n",
    "vectorized_data = embedded_data.withColumn(\"features\", extract_vector_udf(col(\"sentence_embeddings\")))\n",
    "\n",
    "# 5. Chuyển về Pandas để tính cosine similarity\n",
    "pd_data = vectorized_data.select(\"text\", \"features\").toPandas()\n",
    "pd_data[\"features\"] = pd_data[\"features\"].apply(lambda v: v.toArray())\n",
    "\n",
    "# 6. Nhóm theo ngữ nghĩa bằng cosine similarity\n",
    "semantic_groups = []\n",
    "visited = set()\n",
    "threshold = 0.15  # cosine distance dưới ngưỡng này là cùng ngữ nghĩa\n",
    "\n",
    "for idx, (text_i, vec_i) in enumerate(zip(pd_data[\"text\"], pd_data[\"features\"])):\n",
    "    if idx in visited:\n",
    "        continue\n",
    "    group = [text_i]\n",
    "    visited.add(idx)\n",
    "    for jdx in range(idx + 1, len(pd_data)):\n",
    "        if jdx in visited:\n",
    "            continue\n",
    "        dist = cosine(vec_i, pd_data[\"features\"][jdx])\n",
    "        if dist < threshold:\n",
    "            group.append(pd_data[\"text\"][jdx])\n",
    "            visited.add(jdx)\n",
    "    semantic_groups.append(group)\n",
    "\n",
    "# 7. Sắp xếp và in nhóm ngữ nghĩa theo số lượng câu giảm dần\n",
    "semantic_groups = sorted(semantic_groups, key=len, reverse=True)\n",
    "\n",
    "print(\"Các nhóm câu hỏi trùng ngữ nghĩa (sắp xếp theo số lượng):\")\n",
    "for i, group in enumerate(semantic_groups):\n",
    "    print(f\"\\nNhóm {i+1} (số lượng: {len(group)}):\")\n",
    "    for q in group:\n",
    "        print(f\"- {q}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
