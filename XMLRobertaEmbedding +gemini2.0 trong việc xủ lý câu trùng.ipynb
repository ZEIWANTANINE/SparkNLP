{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41790a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark spark-nlp pandas matplotlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc4013",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf, lower, trim, regexp_replace, collect_list\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import os\n",
    "\n",
    "# Cài đặt Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, XlmRoBertaEmbeddings\n",
    "\n",
    "# Khởi tạo SparkSession kết nối đến cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP Clustering\") \\\n",
    "    .master(\"spark://172.18.0.3:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "api_key = \"\"\n",
    "\n",
    "# URL của API Gemini 2.5 (cần thay thế bằng endpoint chính xác của Gemini 2.5)\n",
    "url = \"https://gemini.googleapis.com/v1/generate\"\n",
    "\n",
    "# Đọc file JSONL\n",
    "input_file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "df = spark.read.option(\"multiLine\", False).json(input_file_path)\n",
    "\n",
    "# Trích xuất các câu hỏi từ user\n",
    "user_questions = df.select(explode(\"messages\").alias(\"msg\")) \\\n",
    "    .filter(col(\"msg.role\") == \"user\") \\\n",
    "    .select(col(\"msg.content\").alias(\"text\")) \\\n",
    "    .filter(col(\"text\").isNotNull())\n",
    "\n",
    "# Kiểm tra số lượng câu hỏi\n",
    "print(f\"Số câu hỏi từ người dùng: {user_questions.count()}\")\n",
    "\n",
    "# NLP Pipeline\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "embeddings = XlmRoBertaEmbeddings.pretrained(\"xlmroberta_embeddings_afriberta_base\", \"xx\") \\\n",
    "    .setInputCols([\"document\", \"token\"]).setOutputCol(\"embeddings\")\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, embeddings])\n",
    "model = pipeline.fit(user_questions)\n",
    "embedded_data = model.transform(user_questions)\n",
    "\n",
    "# Tính trung bình vector\n",
    "def avg_embeddings(embeddings):\n",
    "    if embeddings:\n",
    "        avg = np.mean([e['embeddings'] for e in embeddings], axis=0)\n",
    "        return Vectors.dense(avg.tolist())\n",
    "    return Vectors.dense([])\n",
    "\n",
    "avg_embeddings_udf = udf(avg_embeddings, VectorUDT())\n",
    "vectorized_data = embedded_data.withColumn(\"features\", avg_embeddings_udf(col(\"embeddings\")))\n",
    "\n",
    "# Chuẩn hóa vector\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(vectorized_data)\n",
    "scaled_data = scaler_model.transform(vectorized_data)\n",
    "\n",
    "# KMeans clustering\n",
    "k = 5  # Số chủ đề bạn muốn chia, có thể điều chỉnh\n",
    "kmeans = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", k=k)\n",
    "kmeans_model = kmeans.fit(scaled_data)\n",
    "clustered_data = kmeans_model.transform(scaled_data)\n",
    "\n",
    "# Kiểm tra số lượng câu hỏi trong mỗi cluster\n",
    "cluster_count = clustered_data.groupBy(\"cluster\").count().orderBy(\"cluster\")\n",
    "cluster_count.show()\n",
    "\n",
    "# Gom nhóm các câu hỏi theo cluster\n",
    "topics_df = clustered_data.groupBy(\"cluster\") \\\n",
    "    .agg(collect_list(\"text\").alias(\"questions\")) \\\n",
    "    .orderBy(\"cluster\")\n",
    "\n",
    "topics = topics_df.collect()\n",
    "\n",
    "# Hàm gán tên chủ đề cho mỗi cụm bằng cách sử dụng LLM\n",
    "def generate_topic_name_with_gemini(questions):\n",
    "    prompt = \"Dưới đây là một số câu hỏi của người dùng, hãy tạo ra một tên chủ đề ngắn gọn và ngữ nghĩa cho chúng:\\n\"\n",
    "    prompt += \"\\n\".join(questions)  # Thêm các câu hỏi vào prompt\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gemini-2.5\",  # Chỉ định mô hình Gemini 2.5\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 50,  # Giới hạn số lượng token cho tên chủ đề\n",
    "        \"temperature\": 1.0,  # Độ sáng tạo của kết quả\n",
    "    }\n",
    "\n",
    "    # Gửi yêu cầu đến API Gemini\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result['text'].strip()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return \"Không thể tạo tên chủ đề\"\n",
    "\n",
    "# Gán tên chủ đề và gom lại\n",
    "results = []\n",
    "for row in topics:\n",
    "    cluster_id = row[\"cluster\"]\n",
    "    questions = row[\"questions\"]\n",
    "    topic_name = generate_topic_name_with_gemini(questions)\n",
    "    \n",
    "    # Đảm bảo không trùng tên\n",
    "    while topic_name in [r[0] for r in results]:\n",
    "        topic_name += \"_\"\n",
    "    \n",
    "    results.append([topic_name, questions])\n",
    "\n",
    "# Kiểm tra tên chủ đề và câu hỏi\n",
    "for topic in results:\n",
    "    print(f\"Chủ đề: {topic[0]}\")\n",
    "    print(f\"Các câu hỏi: {topic[1][:5]}\")  # Hiển thị 5 câu hỏi đầu tiên trong chủ đề\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Chuyển kết quả thành DataFrame và lưu vào file CSV\n",
    "df_results = pd.DataFrame(results, columns=[\"Topic Name\", \"Questions\"])\n",
    "df_results.to_csv(\"/opt/workspace/clustered_topics.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Đã phân cụm và lưu kết quả vào: /opt/workspace/clustered_topics.csv\")\n",
    "\n",
    "# Kiểm tra số lượng chủ đề và câu hỏi trong kết quả\n",
    "print(f\"Số lượng chủ đề trong kết quả: {len(results)}\")\n",
    "print(f\"Số câu hỏi trong kết quả: {df_results.shape[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
