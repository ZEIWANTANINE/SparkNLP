{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053574d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install spark-nlp scikit-learn openai pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcd4b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, BertEmbeddings\n",
    "\n",
    "# âš™ï¸ Khá»Ÿi táº¡o Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# ğŸ“¥ Äá»c dá»¯ liá»‡u tá»« file JSONL\n",
    "file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# ğŸ“Œ TrÃ­ch cÃ¢u há»i tá»« cÃ¡c tin nháº¯n cá»§a user\n",
    "questions = [\n",
    "    m[\"content\"].strip()\n",
    "    for item in data\n",
    "    for m in item.get(\"messages\", [])\n",
    "    if m[\"role\"] == \"user\" and m.get(\"content\")\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“¦ Tá»•ng sá»‘ cÃ¢u há»i: {len(questions)}\")\n",
    "\n",
    "# Táº¡o DataFrame Spark tá»« danh sÃ¡ch cÃ¢u há»i\n",
    "df = spark.createDataFrame([(q,) for q in questions], [\"question\"])\n",
    "\n",
    "# âœ³ï¸ Táº¡o Spark NLP pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"question\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert = BertEmbeddings.pretrained(\"bert_base_multilingual_cased\", \"xx\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\") \\\n",
    "    .setCaseSensitive(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, bert])\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# ğŸ”¢ TrÃ­ch embedding trung bÃ¬nh má»—i cÃ¢u\n",
    "def extract_avg_embedding(row):\n",
    "    vecs = [emb.embeddings for emb in row['embeddings']]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(768)\n",
    "\n",
    "embeddings = result.select(\"embeddings\").rdd.map(extract_avg_embedding).collect()\n",
    "\n",
    "# Chuyá»ƒn sang numpy array\n",
    "embedding_matrix = np.array(embeddings)\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ğŸ” TÃ­nh cosine similarity vÃ  chuáº©n hÃ³a khoáº£ng cÃ¡ch\n",
    "similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "distance_matrix = 1 - np.clip(similarity_matrix, 0, 1)\n",
    "\n",
    "# ğŸ§  Gom nhÃ³m báº±ng DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=2, metric=\"precomputed\")\n",
    "labels = clustering.fit_predict(distance_matrix)\n",
    "\n",
    "# ğŸ“Š Gom nhÃ³m cÃ¡c cÃ¢u há»i trÃ¹ng\n",
    "groups = {}\n",
    "for label, question in zip(labels, questions):\n",
    "    if label == -1:\n",
    "        continue  # Bá» noise\n",
    "    groups.setdefault(label, []).append(question)\n",
    "\n",
    "# ğŸ“‹ In káº¿t quáº£ nhÃ³m\n",
    "for i, (k, g) in enumerate(groups.items()):\n",
    "    print(f\"\\nğŸ§© NhÃ³m {i+1} ({len(g)} cÃ¢u):\")\n",
    "    for q in g:\n",
    "        print(\" -\", q)\n",
    "\n",
    "# ğŸ“ˆ Thá»‘ng kÃª táº§n suáº¥t\n",
    "print(\"\\nğŸ“Š Táº§n suáº¥t cÃ¡c cÃ¢u há»i:\")\n",
    "for question, freq in Counter(questions).most_common():\n",
    "    print(f\"{question} | {freq} láº§n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
