{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053574d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install spark-nlp scikit-learn openai pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcd4b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, BertEmbeddings\n",
    "\n",
    "# ⚙️ Khởi tạo Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# 📥 Đọc dữ liệu từ file JSONL\n",
    "file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 📌 Trích câu hỏi từ các tin nhắn của user\n",
    "questions = [\n",
    "    m[\"content\"].strip()\n",
    "    for item in data\n",
    "    for m in item.get(\"messages\", [])\n",
    "    if m[\"role\"] == \"user\" and m.get(\"content\")\n",
    "]\n",
    "\n",
    "print(f\"📦 Tổng số câu hỏi: {len(questions)}\")\n",
    "\n",
    "# Tạo DataFrame Spark từ danh sách câu hỏi\n",
    "df = spark.createDataFrame([(q,) for q in questions], [\"question\"])\n",
    "\n",
    "# ✳️ Tạo Spark NLP pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"question\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert = BertEmbeddings.pretrained(\"bert_base_multilingual_cased\", \"xx\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\") \\\n",
    "    .setCaseSensitive(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, bert])\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# 🔢 Trích embedding trung bình mỗi câu\n",
    "def extract_avg_embedding(row):\n",
    "    vecs = [emb.embeddings for emb in row['embeddings']]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(768)\n",
    "\n",
    "embeddings = result.select(\"embeddings\").rdd.map(extract_avg_embedding).collect()\n",
    "\n",
    "# Chuyển sang numpy array\n",
    "embedding_matrix = np.array(embeddings)\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# 🔍 Tính cosine similarity và chuẩn hóa khoảng cách\n",
    "similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "distance_matrix = 1 - np.clip(similarity_matrix, 0, 1)\n",
    "\n",
    "# 🧠 Gom nhóm bằng DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=2, metric=\"precomputed\")\n",
    "labels = clustering.fit_predict(distance_matrix)\n",
    "\n",
    "# 📊 Gom nhóm các câu hỏi trùng\n",
    "groups = {}\n",
    "for label, question in zip(labels, questions):\n",
    "    if label == -1:\n",
    "        continue  # Bỏ noise\n",
    "    groups.setdefault(label, []).append(question)\n",
    "\n",
    "# 📋 In kết quả nhóm\n",
    "for i, (k, g) in enumerate(groups.items()):\n",
    "    print(f\"\\n🧩 Nhóm {i+1} ({len(g)} câu):\")\n",
    "    for q in g:\n",
    "        print(\" -\", q)\n",
    "\n",
    "# 📈 Thống kê tần suất\n",
    "print(\"\\n📊 Tần suất các câu hỏi:\")\n",
    "for question, freq in Counter(questions).most_common():\n",
    "    print(f\"{question} | {freq} lần\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
