{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aaf416",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark==3.4.1 spark-nlp pandas matplotlib scikit_learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9220ae8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf, lower, trim, regexp_replace, count, split\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import collect_list\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, XlmRoBertaEmbeddings\n",
    "\n",
    "# B·∫Øt ƒë·∫ßu t√≠nh gi·ªù to√†n b·ªô pipeline\n",
    "total_start = time.time()\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark NLP\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP Clustering\") \\\n",
    "    .master(\"spark://172.18.0.2:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparknlp.start(spark)\n",
    "\n",
    "\n",
    "# ƒê·ªçc file JSONL\n",
    "input_file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "df = spark.read.option(\"multiLine\", False).json(input_file_path)\n",
    "\n",
    "# Tr√≠ch xu·∫•t c√¢u h·ªèi t·ª´ user\n",
    "user_questions = df.select(explode(\"messages\").alias(\"msg\")) \\\n",
    "    .filter(col(\"msg.role\") == \"user\") \\\n",
    "    .select(col(\"msg.content\").alias(\"text\")) \\\n",
    "    .filter(col(\"text\").isNotNull())\n",
    "\n",
    "# NLP pipeline\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "embeddings = XlmRoBertaEmbeddings.pretrained(\"xlm_roberta_base\", \"xx\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\") \\\n",
    "    .setCaseSensitive(False)\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, embeddings])\n",
    "\n",
    "# Ch·∫°y NLP pipeline\n",
    "start = time.time()\n",
    "model = pipeline.fit(user_questions)\n",
    "embedded_data = model.transform(user_questions)\n",
    "print(f\"‚è± NLP Embedding Pipeline completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# UDF t√≠nh trung b√¨nh embedding\n",
    "def avg_embeddings(embeddings):\n",
    "    try:\n",
    "        if embeddings and len(embeddings) > 0:\n",
    "            vecs = [e['embeddings'] for e in embeddings if e['embeddings']]\n",
    "            if vecs:\n",
    "                avg = np.mean(vecs, axis=0)\n",
    "                return Vectors.dense(avg.tolist())\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "avg_embeddings_udf = udf(avg_embeddings, VectorUDT())\n",
    "\n",
    "# Vector h√≥a v√† chu·∫©n h√≥a\n",
    "start = time.time()\n",
    "vectorized_data = embedded_data.withColumn(\"features\", avg_embeddings_udf(col(\"embeddings\")))\n",
    "vectorized_data = vectorized_data.filter(col(\"features\").isNotNull())\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(vectorized_data)\n",
    "scaled_data = scaler_model.transform(vectorized_data)\n",
    "print(f\"‚è± Feature scaling completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# KMeans clustering\n",
    "start = time.time()\n",
    "if scaled_data.count() >= 5:\n",
    "    kmeans = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", k=5)\n",
    "    kmeans_model = kmeans.fit(scaled_data)\n",
    "    clustered_data = kmeans_model.transform(scaled_data)\n",
    "    print(f\"‚è± KMeans clustering completed in {time.time() - start:.2f} seconds\")\n",
    "else:\n",
    "    raise Exception(\"‚ùó Kh√¥ng ƒë·ªß d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ ph√¢n c·ª•m KMeans (y√™u c·∫ßu >= 5).\")\n",
    "\n",
    "# Chu·∫©n h√≥a vƒÉn b·∫£n\n",
    "normalized_data = clustered_data.withColumn(\n",
    "    \"normalized_text\",\n",
    "    trim(lower(regexp_replace(col(\"text\"), \"[\\\\p{Punct}]\", \"\")))\n",
    ")\n",
    "\n",
    "# ƒê·∫øm t·∫ßn su·∫•t c√¢u h·ªèi\n",
    "start = time.time()\n",
    "grouped = normalized_data.groupBy(\"normalized_text\", \"cluster\") \\\n",
    "    .agg(count(\"*\").alias(\"frequency\")) \\\n",
    "    .orderBy(col(\"frequency\").desc())\n",
    "print(\"üìã Danh s√°ch t·∫•t c·∫£ c√¢u h·ªèi sau khi chu·∫©n h√≥a v√† ph√¢n c·ª•m (theo t·∫ßn su·∫•t):\")\n",
    "grouped.show(truncate=False, n=50)\n",
    "print(f\"‚è± Counting frequency completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì t·∫ßn su·∫•t theo cluster\n",
    "cluster_counts = grouped.groupBy(\"cluster\").sum(\"frequency\") \\\n",
    "    .withColumnRenamed(\"sum(frequency)\", \"count\") \\\n",
    "    .orderBy(\"cluster\") \\\n",
    "    .toPandas()\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(cluster_counts[\"cluster\"], cluster_counts[\"count\"], color=\"teal\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.title(\"Semantic Question Clustering Frequency\")\n",
    "plt.xticks(cluster_counts[\"cluster\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# T√≠nh medoid cho m·ªói c·ª•m\n",
    "start = time.time()\n",
    "clusters = clustered_data.select(\"cluster\", \"text\", \"features\") \\\n",
    "    .groupBy(\"cluster\") \\\n",
    "    .agg(\n",
    "        collect_list(\"text\").alias(\"questions\"),\n",
    "        collect_list(\"features\").alias(\"features_list\")\n",
    "    ).collect()\n",
    "\n",
    "def get_medoid_question(questions, features):\n",
    "    vecs = np.array([v.toArray() for v in features])\n",
    "    dists = cosine_distances(vecs)\n",
    "    total_dists = dists.sum(axis=1)\n",
    "    medoid_idx = np.argmin(total_dists)\n",
    "    return questions[medoid_idx]\n",
    "\n",
    "topic_table = []\n",
    "for row in clusters:\n",
    "    cluster_id = row[\"cluster\"]\n",
    "    questions = row[\"questions\"]\n",
    "    features = row[\"features_list\"]\n",
    "    topic = get_medoid_question(questions, features)\n",
    "    frequency = len(questions)\n",
    "    topic_table.append((cluster_id, topic, frequency))\n",
    "\n",
    "topic_df = pd.DataFrame(topic_table, columns=[\"Cluster\", \"Topic\", \"Frequency\"])\n",
    "topic_df = topic_df.sort_values(\"Cluster\")\n",
    "print(\"\\nüìå Ch·ªß ƒë·ªÅ ƒë·∫°i di·ªán cho t·ª´ng c·ª•m:\")\n",
    "print(topic_df.to_string(index=False))\n",
    "print(f\"‚è± Topic medoid selection completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# G√°n nh√£n th·ªß c√¥ng theo ch·ªß ƒë·ªÅ\n",
    "def classify_topic(text):\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in [\"b√°o h·ªèng\", \"s·ª± c·ªë\", \"b·ªã h·ªèng\", \"tr·∫°ng th√°i h·ªèng\", \"h·ªèng thi·∫øt b·ªã\"]):\n",
    "        return 1\n",
    "    elif any(word in text for word in [\"b·∫£o d∆∞·ª°ng\", \"l·ªãch b·∫£o d∆∞·ª°ng\", \"tr·∫°ng th√°i b·∫£o d∆∞·ª°ng\"]):\n",
    "        return 2\n",
    "    elif any(word in text for word in [\"ƒëi·ªÅu chuy·ªÉn\", \"chuy·ªÉn thi·∫øt b·ªã\", \"chuy·ªÉn ƒë·∫øn\", \"chuy·ªÉn ƒëi\"]):\n",
    "        return 3\n",
    "    elif any(word in text for word in [\"khu v·ª±c\", \"qu·∫£n l√Ω\", \"nh√¢n s·ª±\", \"ch·ª©c v·ª•\", \"ng∆∞·ªùi qu·∫£n l√Ω\", \"th√¥ng tin c√° nh√¢n\"]):\n",
    "        return 4\n",
    "    elif any(word in text for word in [\"thi·∫øt b·ªã\", \"t√†i s·∫£n\", \"lo·∫°i thi·∫øt b·ªã\", \"lo·∫°i t√†i s·∫£n\", \"ch·ª©a t√†i s·∫£n\"]):\n",
    "        return 5\n",
    "    else:\n",
    "        return 0  # Kh√¥ng x√°c ƒë·ªãnh\n",
    "\n",
    "classify_topic_udf = udf(classify_topic)\n",
    "final_df = normalized_data.withColumn(\"classified_topic\", classify_topic_udf(col(\"normalized_text\")))\n",
    "\n",
    "# Th·ªëng k√™ theo nh√£n ph√¢n lo·∫°i ch·ªß ƒë·ªÅ\n",
    "final_stats = final_df.groupBy(\"classified_topic\").count().orderBy(\"classified_topic\")\n",
    "print(\"\\nüìä Th·ªëng k√™ s·ªë l∆∞·ª£ng c√¢u h·ªèi theo ch·ªß ƒë·ªÅ ph√¢n lo·∫°i:\")\n",
    "final_stats.show()\n",
    "\n",
    "# üîΩ Xu·∫•t ra file CSV v√† Excel\n",
    "export_df = final_df.select(\"text\", \"normalized_text\", \"cluster\", \"classified_topic\").toPandas()\n",
    "export_df.to_csv(\"/opt/workspace/clustered_questions.csv\", index=False, encoding='utf-8-sig')\n",
    "export_df.to_excel(\"/opt/workspace/clustered_questions.xlsx\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ xu·∫•t d·ªØ li·ªáu c√¢u h·ªèi ra file:\")\n",
    "print(\"   üìÑ /opt/workspace/clustered_questions.csv\")\n",
    "print(\"   üìÑ /opt/workspace/clustered_questions.xlsx\")\n",
    "\n",
    "# T·ªïng th·ªùi gian\n",
    "print(f\"\\nüöÄ T·ªïng th·ªùi gian to√†n b·ªô pipeline: {time.time() - total_start:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
