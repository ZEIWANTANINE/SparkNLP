{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af647992",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyspark spark-nlp pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365854f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf, lower, trim, regexp_replace, count, split\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler, CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import collect_list\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DeBertaEmbeddings\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSONL\n",
    "input_file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "df = spark.read.option(\"multiLine\", False).json(input_file_path)\n",
    "\n",
    "# Tr√≠ch xu·∫•t c√¢u h·ªèi t·ª´ role l√† \"user\"\n",
    "user_questions = df.select(explode(\"messages\").alias(\"msg\")) \\\n",
    "    .filter(col(\"msg.role\") == \"user\") \\\n",
    "    .select(col(\"msg.content\").alias(\"text\")) \\\n",
    "    .filter(col(\"text\").isNotNull())\n",
    "\n",
    "# T·∫°o pipeline NLP\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = DeBertaEmbeddings.pretrained(\"deberta_embeddings_spm_vie\", \"vie\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\") \\\n",
    "    .setCaseSensitive(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, embeddings])\n",
    "\n",
    "# Ch·∫°y pipeline\n",
    "model = pipeline.fit(user_questions)\n",
    "embedded_data = model.transform(user_questions)\n",
    "\n",
    "# UDF ƒë·ªÉ t√≠nh vector trung b√¨nh t·ª´ embedding\n",
    "def avg_embeddings(embeddings):\n",
    "    if embeddings:\n",
    "        avg = np.mean([e['embeddings'] for e in embeddings], axis=0)\n",
    "        return Vectors.dense(avg.tolist())\n",
    "    return Vectors.dense([])\n",
    "\n",
    "avg_embeddings_udf = udf(avg_embeddings, VectorUDT())\n",
    "\n",
    "# Chuy·ªÉn th√†nh vector ƒë·∫∑c tr∆∞ng\n",
    "vectorized_data = embedded_data.withColumn(\"features\", avg_embeddings_udf(col(\"embeddings\")))\n",
    "\n",
    "# Chu·∫©n h√≥a vector\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(vectorized_data)\n",
    "scaled_data = scaler_model.transform(vectorized_data)\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", k=3)\n",
    "kmeans_model = kmeans.fit(scaled_data)\n",
    "clustered_data = kmeans_model.transform(scaled_data)\n",
    "\n",
    "# Chu·∫©n h√≥a text: ch·ªâ lo·∫°i d·∫•u c√¢u, gi·ªØ nguy√™n ti·∫øng Vi·ªát\n",
    "normalized_data = clustered_data.withColumn(\n",
    "    \"normalized_text\",\n",
    "    trim(lower(regexp_replace(col(\"text\"), \"[\\\\p{Punct}]\", \"\")))\n",
    ")\n",
    "\n",
    "# ƒê·∫øm t·∫ßn su·∫•t c√°c c√¢u h·ªèi sau khi normalize\n",
    "grouped = normalized_data.groupBy(\"normalized_text\", \"cluster\") \\\n",
    "    .agg(count(\"*\").alias(\"frequency\")) \\\n",
    "    .orderBy(col(\"frequency\").desc())\n",
    "\n",
    "# L·∫•y 10 c√¢u h·ªèi c√≥ t·∫ßn su·∫•t xu·∫•t hi·ªán nhi·ªÅu nh·∫•t\n",
    "top_10 = grouped.limit(10)\n",
    "print(\"Top 10 c√¢u h·ªèi xu·∫•t hi·ªán nhi·ªÅu nh·∫•t:\")\n",
    "top_10.show(truncate=False)\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì t·∫ßn su·∫•t theo c·ª•m\n",
    "cluster_counts = grouped.groupBy(\"cluster\").sum(\"frequency\") \\\n",
    "    .withColumnRenamed(\"sum(frequency)\", \"count\") \\\n",
    "    .orderBy(\"cluster\") \\\n",
    "    .toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(cluster_counts[\"cluster\"], cluster_counts[\"count\"], color=\"teal\")\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.title(\"Semantic Question Clustering Frequency\")\n",
    "plt.xticks(cluster_counts[\"cluster\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# üîç Tr√≠ch xu·∫•t t·ª´ kh√≥a c·ªßa t·ª´ng cluster\n",
    "# ================================\n",
    "print(\"\\nT·ª™ KH√ìA ƒê·∫†I DI·ªÜN CHO M·ªñI CLUSTER:\\n\")\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu token h√≥a cho t·∫•t c·∫£\n",
    "tokenized_data = normalized_data.withColumn(\n",
    "    \"tokens\", split(col(\"normalized_text\"), \" \")\n",
    ")\n",
    "\n",
    "# R√∫t t·ª´ kh√≥a cho t·ª´ng cluster\n",
    "for i in range(kmeans.getK()):\n",
    "    print(f\"=== Cluster {i} ===\")\n",
    "    cluster_df = tokenized_data.filter(col(\"cluster\") == i)\n",
    "\n",
    "    # T√≠nh TF cho c·ª•m\n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"keyword_features\", vocabSize=10)\n",
    "    cv_model = cv.fit(cluster_df)\n",
    "    top_keywords = cv_model.vocabulary\n",
    "    print(\"Top keywords:\", top_keywords)\n",
    "    print()\n",
    "# Gom c√¢u h·ªèi v√† embeddings theo cluster\n",
    "clusters = clustered_data.select(\"cluster\", \"text\", \"features\") \\\n",
    "    .groupBy(\"cluster\") \\\n",
    "    .agg(\n",
    "        collect_list(\"text\").alias(\"questions\"),\n",
    "        collect_list(\"features\").alias(\"features_list\")\n",
    "    ).collect()\n",
    "\n",
    "# H√†m ch·ªçn c√¢u ƒë·∫°i di·ªán (medoid) cho m·ªói cluster\n",
    "def get_medoid_question(questions, features):\n",
    "    vecs = np.array([v.toArray() for v in features])\n",
    "    dists = cosine_distances(vecs)\n",
    "    total_dists = dists.sum(axis=1)\n",
    "    medoid_idx = np.argmin(total_dists)\n",
    "    return questions[medoid_idx]\n",
    "\n",
    "# T·∫°o b·∫£ng ch·ªß ƒë·ªÅ ƒë·∫°i di·ªán\n",
    "topic_table = []\n",
    "for row in clusters:\n",
    "    cluster_id = row[\"cluster\"]\n",
    "    questions = row[\"questions\"]\n",
    "    features = row[\"features_list\"]\n",
    "    topic = get_medoid_question(questions, features)\n",
    "    frequency = len(questions)\n",
    "    topic_table.append((cluster_id, topic, frequency))\n",
    "\n",
    "# Chuy·ªÉn sang pandas ƒë·ªÉ in ra b·∫£ng\n",
    "import pandas as pd\n",
    "topic_df = pd.DataFrame(topic_table, columns=[\"Cluster\", \"Topic\", \"Frequency\"])\n",
    "topic_df = topic_df.sort_values(\"Cluster\")\n",
    "print(\"\\nüìå Ch·ªß ƒë·ªÅ ƒë·∫°i di·ªán cho t·ª´ng c·ª•m:\")\n",
    "print(topic_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
