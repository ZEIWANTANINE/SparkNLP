{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e131790",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade pyspark spark-nlp pandas matplotlib scipy google-generativeai pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a418eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DeBertaEmbeddings, SentenceEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import google.generativeai as genai  # ‚úÖ Gemini API\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- B·∫Øt ƒë·∫ßu t√≠nh th·ªùi gian ---\n",
    "total_start = time.time()\n",
    "\n",
    "# --- 1. Kh·ªüi t·∫°o Spark NLP ---\n",
    "t1 = time.time()\n",
    "spark = sparknlp.start()\n",
    "print(f\"‚úÖ Kh·ªüi t·∫°o Spark NLP: {time.time() - t1:.2f} gi√¢y\")\n",
    "\n",
    "# --- 2. ƒê·ªçc d·ªØ li·ªáu JSONL ---\n",
    "t2 = time.time()\n",
    "input_file_path = \"/opt/workspace/gen_1604_formated.jsonl\"\n",
    "df = spark.read.option(\"multiLine\", False).json(input_file_path)\n",
    "user_questions = df.select(explode(\"messages\").alias(\"msg\")) \\\n",
    "    .filter(col(\"msg.role\") == \"user\") \\\n",
    "    .select(col(\"msg.content\").alias(\"text\")) \\\n",
    "    .filter(col(\"text\").isNotNull())\n",
    "print(f\"‚úÖ ƒê·ªçc d·ªØ li·ªáu: {time.time() - t2:.2f} gi√¢y\")\n",
    "\n",
    "# --- 3. Pipeline NLP ---\n",
    "t3 = time.time()\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "embeddings = DeBertaEmbeddings.pretrained(\"deberta_embeddings_spm_vie\", \"vie\") \\\n",
    "    .setInputCols([\"document\", \"token\"]).setOutputCol(\"word_embeddings\")\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"word_embeddings\"]).setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, embeddings, sentence_embeddings])\n",
    "model = pipeline.fit(user_questions)\n",
    "embedded_data = model.transform(user_questions)\n",
    "print(f\"‚úÖ NLP Embedding: {time.time() - t3:.2f} gi√¢y\")\n",
    "\n",
    "# --- 4. Tr√≠ch xu·∫•t vector ---\n",
    "t4 = time.time()\n",
    "def extract_vector(annot):\n",
    "    if annot and isinstance(annot, list) and 'embeddings' in annot[0]:\n",
    "        return Vectors.dense(annot[0]['embeddings'])\n",
    "    return Vectors.dense([0.0] * 768)\n",
    "\n",
    "extract_vector_udf = udf(extract_vector, VectorUDT())\n",
    "vectorized_data = embedded_data.withColumn(\"features\", extract_vector_udf(col(\"sentence_embeddings\")))\n",
    "print(f\"‚úÖ Tr√≠ch xu·∫•t vector: {time.time() - t4:.2f} gi√¢y\")\n",
    "\n",
    "# --- 5. Chuy·ªÉn v·ªÅ Pandas ---\n",
    "t5 = time.time()\n",
    "pd_data = vectorized_data.select(\"text\", \"features\").toPandas()\n",
    "pd_data[\"features\"] = pd_data[\"features\"].apply(lambda v: v.toArray())\n",
    "print(f\"‚úÖ Chuy·ªÉn sang Pandas: {time.time() - t5:.2f} gi√¢y\")\n",
    "\n",
    "# --- 6. Nh√≥m ng·ªØ nghƒ©a ---\n",
    "t6 = time.time()\n",
    "semantic_groups = []\n",
    "visited = set()\n",
    "threshold = 0.15\n",
    "for idx, (text_i, vec_i) in enumerate(zip(pd_data[\"text\"], pd_data[\"features\"])):\n",
    "    if idx in visited:\n",
    "        continue\n",
    "    group = [text_i]\n",
    "    visited.add(idx)\n",
    "    for jdx in range(idx + 1, len(pd_data)):\n",
    "        if jdx in visited:\n",
    "            continue\n",
    "        dist = cosine(vec_i, pd_data[\"features\"][jdx])\n",
    "        if dist < threshold:\n",
    "            group.append(pd_data[\"text\"][jdx])\n",
    "            visited.add(jdx)\n",
    "    semantic_groups.append(group)\n",
    "semantic_groups = sorted(semantic_groups, key=len, reverse=True)\n",
    "print(f\"‚úÖ Nh√≥m ng·ªØ nghƒ©a: {time.time() - t6:.2f} gi√¢y\")\n",
    "\n",
    "# --- 7. C·∫•u h√¨nh Gemini ---\n",
    "GEMINI_API_KEY = \"AIzaSyBRRCysUg0kCd1rLPA8dt0LwP-BS1hC9SQ\"  # üõ†Ô∏è Thay b·∫±ng key h·ª£p l·ªá\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.5-pro-preview-03-25\")\n",
    "\n",
    "# --- 8. T√°ch nh√≥m theo batch ---\n",
    "def split_groups_into_batches(groups, batch_size=10):\n",
    "    for i in range(0, len(groups), batch_size):\n",
    "        yield groups[i:i+batch_size]\n",
    "\n",
    "# --- 9. G·ªçi Gemini API ---\n",
    "def classify_multiple_groups_with_gemini(groups_batch):\n",
    "    prompt = (\n",
    "        \"B·∫°n h√£y ph√¢n lo·∫°i t·ª´ng nh√≥m c√°c c√¢u h·ªèi d∆∞·ªõi ƒë√¢y v·ªÅ 5 lo·∫°i:\\n\"\n",
    "        \"1. B√°o h·ªèng thi·∫øt b·ªã, s·ª± c·ªë, tr·∫°ng th√°i b√°o h·ªèng thi·∫øt b·ªã.\\n\"\n",
    "        \"2. B·∫£o d∆∞·ª°ng thi·∫øt b·ªã, tr·∫°ng th√°i b·∫£o d∆∞·ª°ng, l·ªãch b·∫£o d∆∞·ª°ng.\\n\"\n",
    "        \"3. ƒêi·ªÅu chuy·ªÉn thi·∫øt b·ªã, thi·∫øt b·ªã ƒë∆∞·ª£c ƒëi·ªÅu chuy·ªÉn ƒëi ƒë√¢u.\\n\"\n",
    "        \"4. V·∫•n ƒë·ªÅ nh√¢n s·ª±, khu v·ª±c, th√¥ng tin c√° nh√¢n, ch·ª©c v·ª•, khu v·ª±c qu·∫£n l√Ω, ng∆∞·ªùi qu·∫£n l√Ω, t√™n ri√™ng.\\n\"\n",
    "        \"5. T√†i s·∫£n, thi·∫øt b·ªã, lo·∫°i t√†i s·∫£n, khu v·ª±c ch·ª©a t√†i s·∫£n.\\n\\n\"\n",
    "        \"Danh s√°ch c√°c nh√≥m c√¢u h·ªèi:\\n\"\n",
    "    )\n",
    "    for group_idx, group_texts in enumerate(groups_batch, 1):\n",
    "        prompt += f\"Nh√≥m {group_idx}:\\n\"\n",
    "        for idx, text in enumerate(group_texts, 1):\n",
    "            prompt += f\"  {idx}. {text}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    prompt += \"H√£y tr·∫£ v·ªÅ danh s√°ch c√°c s·ªë nguy√™n t·ª´ 1 ƒë·∫øn 5, m·ªói s·ªë l√† ph√¢n lo·∫°i cho nh√≥m t∆∞∆°ng ·ª©ng theo th·ª© t·ª± nh√≥m ƒë√£ cho, v√≠ d·ª•: [1, 2, 1, 5, 3,...]\"\n",
    "\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        answer = response.text.strip()\n",
    "        labels = list(map(int, re.findall(r\"[1-5]\", answer)))\n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói g·ªçi Gemini: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 10. Ch·∫°y ph√¢n lo·∫°i theo batch ---\n",
    "t7 = time.time()\n",
    "batch_size = 10\n",
    "all_group_labels = []\n",
    "for batch_idx, batch_groups in enumerate(split_groups_into_batches(semantic_groups, batch_size=batch_size)):\n",
    "    labels = classify_multiple_groups_with_gemini(batch_groups)\n",
    "    if labels:\n",
    "        all_group_labels.extend(labels)\n",
    "    else:\n",
    "        all_group_labels.extend([None] * len(batch_groups))\n",
    "    print(f\"ƒê√£ x·ª≠ l√Ω batch {batch_idx + 1} / {math.ceil(len(semantic_groups) / batch_size)}\")\n",
    "print(f\"‚úÖ G·ªçi Gemini & ph√¢n lo·∫°i: {time.time() - t7:.2f} gi√¢y\")\n",
    "\n",
    "# --- 11. K·∫øt qu·∫£ chi ti·∫øt ---\n",
    "for i, (group, label) in enumerate(zip(semantic_groups, all_group_labels)):\n",
    "    print(f\"\\nNh√≥m {i+1} (Lo·∫°i {label}, s·ªë l∆∞·ª£ng: {len(group)}):\")\n",
    "    for q in group:\n",
    "        print(f\"- {q}\")\n",
    "\n",
    "# --- 12. Th·ªëng k√™ s·ªë l∆∞·ª£ng c√¢u h·ªèi theo lo·∫°i d∆∞·ªõi d·∫°ng b·∫£ng ---\n",
    "category_labels = {\n",
    "    1: \"B√°o h·ªèng thi·∫øt b·ªã\",\n",
    "    2: \"B·∫£o d∆∞·ª°ng thi·∫øt b·ªã\",\n",
    "    3: \"ƒêi·ªÅu chuy·ªÉn thi·∫øt b·ªã\",\n",
    "    4: \"V·∫•n ƒë·ªÅ nh√¢n s·ª±\",\n",
    "    5: \"T√†i s·∫£n / thi·∫øt b·ªã\"\n",
    "}\n",
    "category_stats = defaultdict(int)\n",
    "for group, label in zip(semantic_groups, all_group_labels):\n",
    "    if label is not None:\n",
    "        category_stats[label] += len(group)\n",
    "\n",
    "stats_df = pd.DataFrame([\n",
    "    {\"Lo·∫°i\": i, \"T√™n ph√¢n lo·∫°i\": category_labels[i], \"S·ªë l∆∞·ª£ng c√¢u h·ªèi\": category_stats[i]}\n",
    "    for i in range(1, 6)\n",
    "])\n",
    "print(\"\\n--- Th·ªëng k√™ s·ªë l∆∞·ª£ng c√¢u h·ªèi theo ph√¢n lo·∫°i ---\")\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# --- 13. V·∫Ω bi·ªÉu ƒë·ªì c·ªôt ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    x=\"T√™n ph√¢n lo·∫°i\", \n",
    "    y=\"S·ªë l∆∞·ª£ng c√¢u h·ªèi\", \n",
    "    data=stats_df, \n",
    "    palette=\"Set2\"\n",
    ")\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(\n",
    "        format(p.get_height(), \".0f\"), \n",
    "        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "        ha='center', va='center',\n",
    "        fontsize=11, color='black', \n",
    "        xytext=(0, 10), \n",
    "        textcoords='offset points'\n",
    "    )\n",
    "plt.title(\"Bi·ªÉu ƒë·ªì ph√¢n lo·∫°i c√°c nh√≥m c√¢u h·ªèi\", fontsize=16)\n",
    "plt.xlabel(\"Ph√¢n lo·∫°i\", fontsize=12)\n",
    "plt.ylabel(\"S·ªë l∆∞·ª£ng c√¢u h·ªèi\", fontsize=12)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- T·ªïng k·∫øt th·ªùi gian ---\n",
    "total_end = time.time()\n",
    "print(f\"\\n‚è±Ô∏è T·ªïng th·ªùi gian th·ª±c thi to√†n b·ªô script: {total_end - total_start:.2f} gi√¢y\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
